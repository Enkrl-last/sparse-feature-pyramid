{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65999061",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_PATHS = [\"..\"]\n",
    "import sys\n",
    "for path in PYTHON_PATHS:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "from pytorch_lightning.utilities.parsing import AttributeDict\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "import albumentations\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e413cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f88eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparse_feature_pyramid.model import SparseFeaturePyramidAutoencoder\n",
    "from sparse_feature_pyramid.data import SevenScenesDataModule\n",
    "from sparse_feature_pyramid.utils import UniversalFactory\n",
    "from sparse_feature_pyramid.utils.clearml_figure_reporter import ClearmlFigureReporter\n",
    "\n",
    "factory = UniversalFactory([SparseFeaturePyramidAutoencoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417a6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clearml import Task, Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c51501",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c352ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ToyDataModule] - train subset size 2000\n",
      "[ToyDataModule] - validation dataset size 2000\n"
     ]
    }
   ],
   "source": [
    "# Тренируем на всех сценах\n",
    "image_size = 256\n",
    "data_module_parameters = {\n",
    "    \"batch_size\": 16,\n",
    "    \"num_workers\": 4,\n",
    "    \"image_size\": image_size,\n",
    "    \"scenes\": [\"fire\"], #, \"chess\", \"pumpkin\", \"stairs\", \"heads\", \"office\", \"redkitchen\"],\n",
    "    \"center_crop\": True,\n",
    "    \"random_jitter\": True,\n",
    "    \"random_rotation\": True,\n",
    "    \"root_dataset_path\": \"/home/andrei/media/7scenes\"\n",
    "}\n",
    "\n",
    "scene = data_module_parameters[\"scenes\"][0]\n",
    "data_module = SevenScenesDataModule(**data_module_parameters)\n",
    "model_parameters = AttributeDict(\n",
    "    name=\"SparseFeaturePyramidAutoencoder\",\n",
    "    optimizer=AttributeDict(),\n",
    "    feature_dimensions=[8, 16, 32, 64, 128],\n",
    "    size_loss_koef=(image_size*image_size*3) * (1 / 500000.),\n",
    "    input_dimension=3,\n",
    "    kl_loss_coefficient=1\n",
    ")\n",
    "model = factory.make_from_parameters(model_parameters)\n",
    "model.set_figure_reporter(ClearmlFigureReporter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d3c6d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"/home/andrei/git/sparse-feature-pyramid/notebook/version_14/checkpoints/epoch=89-step=36629.ckpt\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "611072a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(batch, index):\n",
    "    input_image = batch[\"image\"][index].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    input_image = input_image * np.array(data_module._std)[None, None] + np.array(data_module._mean)[None, None]\n",
    "    input_image = np.clip(input_image, 0, 1)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b7a0b",
   "metadata": {},
   "source": [
    "# Extract points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8fec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kapture_localization.matching.matching import MatchPairNnTorch\n",
    "from torch.nn.functional import grid_sample\n",
    "import argparse\n",
    "import numpy as np\n",
    "import imageio\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import kornia\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba264f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/miniconda3/envs/enGAN/lib/python3.9/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def get_keypoint_grid(output, level):\n",
    "    descriptors = output[1][level]\n",
    "    height = descriptors.shape[2]\n",
    "    width = descriptors.shape[3]\n",
    "    batch_size = descriptors.shape[0]\n",
    "    x, y = np.meshgrid(range(height), range(width))\n",
    "    keypoints = np.array([x, y]).transpose(1, 2, 0).astype(np.float32)\n",
    "    keypoints[:, :, 0] = 2 * keypoints[:, :, 0] / (width - 1) - 1\n",
    "    keypoints[:, :, 1] = 2 * keypoints[:, :, 1] / (height - 1) - 1\n",
    "    keypoints = torch.tensor(keypoints)[None]\n",
    "    return torch.repeat_interleave(keypoints, batch_size, dim=0)\n",
    "    \n",
    "def get_numpy_descriptors(descriptors, index):\n",
    "    descriptors = descriptors[index].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    feature_dimension = descriptors.shape[-1]\n",
    "    descriptors = descriptors.reshape(-1, feature_dimension)\n",
    "    descriptors = descriptors / (np.linalg.norm(descriptors, axis=1)[:, None] + 1e-9)\n",
    "    return descriptors\n",
    "\n",
    "def get_interpolated_descriptors(output, index, level, keypoint_grid):\n",
    "    descriptors = grid_sample(output[1][level], keypoint_grid, align_corners=True)\n",
    "    return get_numpy_descriptors(descriptors, index)\n",
    "\n",
    "\n",
    "def get_updated_descriptors(output, index, level):\n",
    "    masked_feature_pyramid = [feature * mask for feature, mask in zip(output[1], output[2])]\n",
    "    x = masked_feature_pyramid[-1]\n",
    "    for i in range(3 - level):\n",
    "        x = model._decoder_blocks[i](x, masked_feature_pyramid[-i - 2])\n",
    "\n",
    "    x = model._decoder_blocks[3 - level]._upsample_conv(x)\n",
    "    x = torch.cat([x, masked_feature_pyramid[level]], dim=1)\n",
    "    return get_numpy_descriptors(x, index)\n",
    "\n",
    "def get_mask(output, index, level):\n",
    "    mask = output[2][level][index][0].detach().cpu().numpy().reshape(-1)\n",
    "    return mask.astype(bool)\n",
    "\n",
    "def get_descriptors(output, index, level):\n",
    "    return get_numpy_descriptors(output[1][level], index)\n",
    "\n",
    "def get_keypoints(output, index, level):\n",
    "    descriptors = output[1][level][index].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    height = descriptors.shape[0]\n",
    "    width = descriptors.shape[1]\n",
    "    x, y = np.meshgrid(range(height), range(width))\n",
    "    keypoints = np.array([x, y]).transpose(1, 2, 0).reshape(-1, 2)\n",
    "    keypoints = keypoints / (width - 1) * (width * 2 ** level - 1)\n",
    "    return keypoints\n",
    "\n",
    "def get_descriptors_and_keypoints(output, index, level):\n",
    "    keypoints = get_keypoints(output, index, level)\n",
    "    descriptors = get_descriptors(output, index, level)\n",
    "    mask = get_mask(output, index, level)\n",
    "    return keypoints[mask], descriptors[mask]\n",
    "\n",
    "def get_interpolated_descriptors_and_keypoints(output, index, level, descriptor_levels):\n",
    "    keypoints = get_keypoints(output, index, level)\n",
    "    keypoint_grid = get_keypoint_grid(output, level)\n",
    "    descriptors = []\n",
    "    for descriptor_level in descriptor_levels:\n",
    "        descriptors.append(get_interpolated_descriptors(output, index, descriptor_level, keypoint_grid))\n",
    "    descriptors = np.concatenate(descriptors, axis=1)\n",
    "    mask = get_mask(output, index, level)\n",
    "    return keypoints[mask], descriptors[mask]\n",
    "\n",
    "def generate_read_function(method, extension='ppm'):\n",
    "    def read_function(seq_name, im_idx):\n",
    "        aux = np.load(os.path.join(dataset_path, seq_name, '%d.%s.%s' % (im_idx, extension, method)), allow_pickle=True)\n",
    "        if top_k is None:\n",
    "            return aux['keypoints'], aux['descriptors']\n",
    "        else:\n",
    "            assert('scores' in aux)\n",
    "            ids = np.argsort(aux['scores'])[-top_k :]\n",
    "            return aux['keypoints'][ids, :], aux['descriptors'][ids, :]\n",
    "    return read_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76992d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/miniconda3/envs/enGAN/lib/python3.9/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "100%|██████████| 648/648 [01:06<00:00,  9.75it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_tensor = transforms.ToTensor()\n",
    "\n",
    "keypoints = []\n",
    "scores = []\n",
    "descriptors = []\n",
    "\n",
    "image_list_file = '/home/andrei/git/d2-net/image_list_hpatches_sequences.txt'\n",
    "newsize = (256, 256)\n",
    "level = 2\n",
    "descript = [2, 3, 4]\n",
    "\n",
    "with open(image_list_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "for line in tqdm(lines, total=len(lines)):\n",
    "    path = \"/home/andrei/git/d2-net/\" + line.strip()\n",
    "\n",
    "    # reshape image\n",
    "    im = Image.open(path)\n",
    "    w, h = im.size\n",
    "    im = im.resize(newsize)\n",
    "    image_orig = np.array(im)\n",
    "    \n",
    "    # normalize image\n",
    "    image = image_orig/256\n",
    "    \n",
    "    # cinvert to proper shape for NN\n",
    "    image = (image - np.array(data_module._mean))/np.array(data_module._std)\n",
    "    image_batch = convert_tensor(image.astype('float32'))[None]\n",
    "    output = model(image_batch)\n",
    "\n",
    "    # obtain kp and descriptors. No scores for now\n",
    "    keypoints, descriptors = get_interpolated_descriptors_and_keypoints(output, 0, level, descript)\n",
    "    scores = np.zeros((keypoints.shape[0], 1))\n",
    "    \n",
    "    # return back to original size\n",
    "    keypoints[:, 0] = keypoints[:, 0] * (w/256)\n",
    "    keypoints[:, 1] = keypoints[:, 1] * (h/256)\n",
    "    \n",
    "    # save each image as npy file\n",
    "    with open(path + '.SFP', 'wb') as output_file:\n",
    "        np.savez(\n",
    "            output_file,\n",
    "            keypoints=keypoints,\n",
    "            scores=scores,\n",
    "            descriptors=descriptors\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
